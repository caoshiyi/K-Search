max_iterations: 120
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration for Metal kernel optimization
llm:
  primary_model: "gemini-3-pro-preview"
  primary_model_weight: 1
  secondary_model: "gemini-3-flash-preview"
  secondary_model_weight: 0
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.6
  top_p: 0.95
  max_tokens: 80000
  timeout: 900

# Specialized prompt for Metal kernel optimization
prompt:
  system_message: |
    You are a code generator. Generate a CUDA kernel implementation optimized for H100 GPU for the following specification.

    Specification:
    Name: mla_paged_decode_h16_ckv512_kpe64_ps1
    Type: mla_paged

    Axes:
      batch_size: variable
      num_qo_heads: constant = 16 (Number of query heads after tensor parallel split (128/8=16).)
      head_dim_ckv: constant = 512
      head_dim_kpe: constant = 64
      page_size: constant = 1
      num_pages: variable (Total number of allocated pages in the KV cache.)
      len_indptr: variable (Length of kv_indptr array.)
      num_kv_indices: variable (Total number of KV page indices.)

    Inputs:
      q_nope: [batch_size, num_qo_heads, head_dim_ckv] (DType.BFLOAT16) - Query tensor without positional encoding component.
      q_pe: [batch_size, num_qo_heads, head_dim_kpe] (DType.BFLOAT16) - Query positional encoding component.
      ckv_cache: [num_pages, page_size, head_dim_ckv] (DType.BFLOAT16) - Compressed key-value cache.
      kpe_cache: [num_pages, page_size, head_dim_kpe] (DType.BFLOAT16) - Key positional encoding cache.
      kv_indptr: [len_indptr] (DType.INT32) - KV page offsets for each sequence. For decode (single-query), we don't need qo_indptr.
      kv_indices: [num_kv_indices] (DType.INT32) - Page indices for KV cache lookups.
      sm_scale: scalar (DType.FLOAT32) - Softmax scale. Default is (1/sqrt(128 + 64) = 1/sqrt(192)), based on head dimensions before matrix absorption.

    Outputs:
      output: [batch_size, num_qo_heads, head_dim_ckv] (DType.BFLOAT16)
      lse: [batch_size, num_qo_heads] (DType.FLOAT32) - The 2-based log-sum-exp of attention logits.

    Constraints:
      len_indptr == batch_size + 1
      num_kv_indices == kv_indptr[-1].item()


    Reference Implementation:
    import math
    import torch


    @torch.no_grad()
    def run(q_nope, q_pe, ckv_cache, kpe_cache, kv_indptr, kv_indices, sm_scale):
        batch_size, num_qo_heads, head_dim_ckv = q_nope.shape
        head_dim_kpe = q_pe.shape[-1]
        page_size = ckv_cache.shape[1]
        len_indptr = kv_indptr.shape[0]
        num_kv_indices = kv_indices.shape[0]

        # Check constants
        assert num_qo_heads == 16
        assert head_dim_ckv == 512
        assert head_dim_kpe == 64
        assert page_size == 1

        # Check constraints
        assert len_indptr == batch_size + 1
        assert num_kv_indices == kv_indptr[-1].item()

        device = q_nope.device

        Kc_all = ckv_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_ckv]
        Kp_all = kpe_cache.squeeze(1).to(torch.float32)  # [num_pages, head_dim_kpe]

        output = torch.zeros(
            (batch_size, num_qo_heads, head_dim_ckv), dtype=torch.bfloat16, device=device
        )
        lse = torch.full((batch_size, num_qo_heads), -float("inf"), dtype=torch.float32, device=device)

        for b in range(batch_size):
            page_beg = int(kv_indptr[b].item())
            page_end = int(kv_indptr[b + 1].item())

            if page_beg >= page_end:
                # No KV cache for this batch element
                output[b].zero_()
                continue

            pages = kv_indices[page_beg:page_end]
            # Derive kv_len from kv_indptr (for page_size=1, num_pages == num_tokens)
            L_tokens = page_end - page_beg

            if L_tokens <= 0 or pages.numel() == 0:
                output[b].zero_()
                continue

            # Pages are token indices for page_size=1
            tok_idx = pages[:L_tokens].to(torch.long)

            Kc = Kc_all[tok_idx]  # [L_tokens, head_dim_ckv]
            Kp = Kp_all[tok_idx]  # [L_tokens, head_dim_kpe]
            qn = q_nope[b].to(torch.float32)  # [num_qo_heads, head_dim_ckv]
            qp = q_pe[b].to(torch.float32)  # [num_qo_heads, head_dim_kpe]

            logits = (qn @ Kc.T) + (qp @ Kp.T)  # [num_qo_heads, L_tokens]
            logits_scaled = logits * sm_scale

            # Compute 2-base LSE
            lse[b] = torch.logsumexp(logits_scaled, dim=-1) / math.log(2.0)

            attn = torch.softmax(logits_scaled, dim=-1)  # [num_qo_heads, L_tokens]
            out = attn @ Kc  # [num_qo_heads, head_dim_ckv]
            output[b] = out.to(torch.bfloat16)

        return {"output": output, "lse": lse}

    Requirements:
    - Write clean, efficient CUDA C++ code optimized for H100 architecture
    - Use proper CUDA syntax and memory management optimized for H100
    - Implement the exact functionality described in the specification
    - The reference code provides the mathematical specification but is unoptimized - your CUDA implementation should match its computational accuracy while delivering high performance
    - Use the definition's tensor shapes, dtypes, and axes information to guide memory access patterns and optimization strategies
    - Optimize for H100 GPU characteristics (memory hierarchy, compute units, etc.)
    - For fixed axis values, optimize specifically for those constants rather than general cases
    - You may use 3rd party libraries (cuBLAS, cuDNN, CUTLASS) when beneficial, but custom implementations often perform better for specialized kernels with known axis constraints

    IMPORTANT: Generate code in XML format with exactly 3 files with these strict names:

    <header_file name="kernel.h">
    - All CUDA kernel function declarations
    - Host function declarations
    - Any necessary struct/type definitions
    - Include guards and necessary headers
    </header_file>

    <cuda_file name="kernel.cu">
    - All __global__ kernel implementations
    - All __device__ helper functions
    - CUDA-specific optimizations and memory patterns
    - Proper error checking and memory management
    </cuda_file>

    <cpp_file name="main.cpp">
    - Host function that launches kernels
    - Memory allocation and data transfer management
    - Device management and error handling
    - Entry point function named "run" that can be called to execute the implementation
    - Handle both args and kwargs properly
    - Move CPU data to GPU, execute kernels, and return results to CPU
    - Include PyTorch C++ extension bindings using PYBIND11_MODULE
    - The "run" function must be exposed to Python through the binding
    - Include proper tensor type conversion between PyTorch tensors and CUDA pointers
    - Include all necessary PyTorch headers: #include <torch/extension.h>
    </cpp_file>

    Code Generation Guidelines:
    1. Use modern CUDA features appropriate for H100
    2. Optimize memory coalescing and reduce bank conflicts
    3. Utilize shared memory effectively for data reuse
    4. Consider occupancy and register usage
    5. Implement proper error checking with cudaGetLastError()
    6. Use appropriate grid and block dimensions for the problem size
    7. Leverage constant memory for frequently accessed read-only data
    8. Use PyTorch tensor API (torch::Tensor) for all tensor arguments in the "run" function
    9. Convert PyTorch tensors to CUDA pointers using .data_ptr<float>() or similar methods
    10. Ensure proper CUDA stream synchronization and error handling
    
    ** You MUST use MMA to utilize the tensor cores on H100!

    Performance targets (lower is better):
    - workload bd2dae14-7bae-4edb-964f-2163accf506e: target_latency_ms <= 0.019
    - workload 84221f45-78f8-4d44-84f6-998153d2c1fa: target_latency_ms <= 0.019
    - workload d0da33e2-2d94-42b5-be8a-09111f9f2649: target_latency_ms <= 0.016
    - workload e417264f-195d-4204-89fa-3ebdb539f1cf: target_latency_ms <= 0.018
    - workload 939f995a-1ab2-4d19-8d94-50f07e73542d: target_latency_ms <= 0.052
    Optimize for overall mean latency across the listed workloads while maintaining correctness.
    
  num_top_programs: 3
  num_diverse_programs: 2

# Database configuration
database:
  db_path: "./openevolve_output/flashinfer_mla_gemini3_run_2"
  population_size: 25
  archive_size: 12
  num_islands: 3
  elite_selection_ratio: 0.3
  exploitation_ratio: 0.65
  exploration_ratio: 0.35

# Evaluator configuration
evaluator:
  timeout: 1200  # 20 minutes for Metal kernel compilation and testing
  parallel_evaluations: 1
  cascade_evaluation: false

# Evolution settings
diff_based_evolution: false
allow_full_rewrites: true
max_code_length: 90000