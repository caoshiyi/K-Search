# EVOLVE-BLOCK-START

<header_file name="kernel.h">
#pragma once
#include <torch/extension.h>
#include <cuda_runtime.h>

void run_mla_decode(
    torch::Tensor q_nope,
    torch::Tensor q_pe,
    torch::Tensor ckv_cache,
    torch::Tensor kpe_cache,
    torch::Tensor kv_indptr,
    torch::Tensor kv_indices,
    torch::Tensor output,
    torch::Tensor lse,
    float sm_scale
);
</header_file>

<cuda_file name="kernel.cu">
#include "kernel.h"
#include <cuda_bf16.h>
#include <cuda_pipeline.h>
#include <mma.h>
#include <cmath>

using namespace nvcuda;

// Tuning params for H100
// Block size 256, 8 warps
constexpr int BLOCK_SIZE = 256;
constexpr int NUM_WARPS = BLOCK_SIZE / 32;

constexpr int NUM_HEADS = 16;
constexpr int HEAD_DIM_CKV = 512;
constexpr int HEAD_DIM_KPE = 64;

constexpr int CHUNK_SIZE = 64; 

// Memory Layout Constants
// Padding to avoid bank conflicts
// CKV: [Chunk, 512]. Stride 512+8=520 elements.
constexpr int PAD_CKV = 8;
constexpr int STRIDE_CKV = HEAD_DIM_CKV + PAD_CKV; // 520
// KPE: [Chunk, 64]. Stride 64+8=72 elements.
constexpr int PAD_KPE = 8;
constexpr int STRIDE_KPE = HEAD_DIM_KPE + PAD_KPE; // 72

// Stride for S_partials (Split-K reduction buffer)
// Each warp stores [16 heads, 64 tokens]. We pad the 64 dimension.
// 64 + 8 = 72 floats
constexpr int S_STRIDE = 72;

struct __align__(128) SharedStorage {
    // Q matrices
    __nv_bfloat16 q_nope[NUM_HEADS * HEAD_DIM_CKV]; // 16KB
    __nv_bfloat16 q_pe[NUM_HEADS * HEAD_DIM_KPE];   // 2KB
    
    // KV Buffers (Double Buffered)
    // Layout: [2 buffers][Chunk][Stride]
    __nv_bfloat16 kc_buf[2][CHUNK_SIZE * STRIDE_CKV]; // ~133KB
    __nv_bfloat16 kp_buf[2][CHUNK_SIZE * STRIDE_KPE]; // ~18KB
    
    // P Matrix for Softmax -> Output accumulation (16 heads x 64 tokens)
    // Row major. Stride 64.
    __nv_bfloat16 p_mat[NUM_HEADS * CHUNK_SIZE]; // 2KB

    // Reused scratch memory
    union {
        // S Partials: Each warp stores a [16, 64] matrix (logically)
        // [NUM_WARPS][16 rows][S_STRIDE cols]
        float s_partials[NUM_WARPS][16 * S_STRIDE]; // ~36KB
        
        // Output exchange buffer for final reduction
        float o_exchange[NUM_HEADS * HEAD_DIM_CKV]; // 32KB
    } scratch;

    // Softmax statistics
    float lse_max[NUM_HEADS];
    float lse_sum[NUM_HEADS];
    float broadcast_alpha[NUM_HEADS];
};

__device__ __forceinline__ void load_q(
    SharedStorage& smem,
    const __nv_bfloat16* __restrict__ qn_g,
    const __nv_bfloat16* __restrict__ qp_g,
    int tid
) {
    const int4* src_qn = reinterpret_cast<const int4*>(qn_g);
    int4* dst_qn = reinterpret_cast<int4*>(smem.q_nope);
    
    // Load 16 * 512 bf16s = 8192 elements. 256 threads -> 32 elems/thread.
    #pragma unroll
    for (int i = 0; i < 4; ++i) {
        int idx = i * BLOCK_SIZE + tid;
        dst_qn[idx] = src_qn[idx];
    }
    
    const int4* src_qp = reinterpret_cast<const int4*>(qp_g);
    int4* dst_qp = reinterpret_cast<int4*>(smem.q_pe);
    // Load 16 * 64 bf16s = 1024 elements. 256 threads -> 4 elems/thread.
    if (tid < 128) { 
        dst_qp[tid] = src_qp[tid];
    }
}

__device__ __forceinline__ void load_kv_chunk(
    SharedStorage& smem,
    int buf_idx,
    const int* __restrict__ kv_indices,
    int valid_rows,
    const __nv_bfloat16* __restrict__ ckv_base,
    const __nv_bfloat16* __restrict__ kpe_base,
    int tid
) {
    // Load CKV: [64, 512]
    // 256 threads.
    #pragma unroll
    for (int i = 0; i < 16; ++i) {
        int job_idx = tid + i * BLOCK_SIZE;
        int row = job_idx >> 6; // / 64
        int col_chunk = job_idx & 63; // % 64
        
        if (row < valid_rows) {
            int page_idx = kv_indices[row];
            const void* src = ckv_base + (long long)page_idx * HEAD_DIM_CKV + col_chunk * 8;
            void* dst = smem.kc_buf[buf_idx] + row * STRIDE_CKV + col_chunk * 8;
            __pipeline_memcpy_async(dst, src, 16);
        }
    }
    
    // Load KPE: [64, 64]
    #pragma unroll
    for (int i = 0; i < 2; ++i) {
        int job_idx = tid + i * BLOCK_SIZE;
        if (job_idx < 512) {
            int row = job_idx >> 3; // / 8
            int col_chunk = job_idx & 7; // % 8
            if (row < valid_rows) {
                int page_idx = kv_indices[row];
                const void* src = kpe_base + (long long)page_idx * HEAD_DIM_KPE + col_chunk * 8;
                void* dst = smem.kp_buf[buf_idx] + row * STRIDE_KPE + col_chunk * 8;
                __pipeline_memcpy_async(dst, src, 16);
            }
        }
    }
}

__device__ __forceinline__ void zero_invalid_rows(
    SharedStorage& smem,
    int buf_idx,
    int valid_rows,
    int tid
) {
    if (valid_rows >= CHUNK_SIZE) return;

    // Zero CKV
    #pragma unroll
    for (int i = 0; i < 16; ++i) { 
        int job_idx = tid + i * BLOCK_SIZE;
        int row = job_idx >> 6;
        int col_chunk = job_idx & 63;
        
        if (row >= valid_rows && row < CHUNK_SIZE) {
             int4* dst = reinterpret_cast<int4*>(smem.kc_buf[buf_idx] + row * STRIDE_CKV + col_chunk * 8);
             *dst = make_int4(0, 0, 0, 0);
        }
    }
    
    // Zero KPE
    #pragma unroll
    for (int i = 0; i < 2; ++i) {
        int job_idx = tid + i * BLOCK_SIZE;
        if (job_idx < 512) {
            int row = job_idx >> 3;
            int col_chunk = job_idx & 7;
            if (row >= valid_rows && row < CHUNK_SIZE) {
                int4* dst = reinterpret_cast<int4*>(smem.kp_buf[buf_idx] + row * STRIDE_KPE + col_chunk * 8);
                *dst = make_int4(0, 0, 0, 0);
            }
        }
    }
}

__global__ __launch_bounds__(256, 1)
void mla_decode_kernel(
    const __nv_bfloat16* __restrict__ q_nope,
    const __nv_bfloat16* __restrict__ q_pe,
    const __nv_bfloat16* __restrict__ ckv_cache,
    const __nv_bfloat16* __restrict__ kpe_cache,
    const int* __restrict__ kv_indptr,
    const int* __restrict__ kv_indices,
    __nv_bfloat16* __restrict__ output,
    float* __restrict__ lse,
    float sm_scale
) {
    extern __shared__ char smem_raw[];
    SharedStorage& smem = *reinterpret_cast<SharedStorage*>(smem_raw);
    
    int batch_idx = blockIdx.y;
    int tid = threadIdx.x;
    int warp_id = tid / 32;

    if (tid < NUM_HEADS) {
        smem.lse_max[tid] = -INFINITY;
        smem.lse_sum[tid] = 0.0f;
    }

    const __nv_bfloat16* qn_g = q_nope + batch_idx * NUM_HEADS * HEAD_DIM_CKV;
    const __nv_bfloat16* qp_g = q_pe + batch_idx * NUM_HEADS * HEAD_DIM_KPE;
    
    load_q(smem, qn_g, qp_g, tid);

    wmma::fragment<wmma::accumulator, 16, 16, 16, float> acc_o[4];
    #pragma unroll
    for (int i=0; i<4; ++i) wmma::fill_fragment(acc_o[i], 0.0f);
    
    __syncthreads();
    
    int page_start = kv_indptr[batch_idx];
    int total_tokens = kv_indptr[batch_idx + 1] - page_start;
    
    if (total_tokens <= 0) {
        if (tid < NUM_HEADS) lse[batch_idx * NUM_HEADS + tid] = -INFINITY;
        for (int i = tid; i < NUM_HEADS * HEAD_DIM_CKV; i += BLOCK_SIZE) {
             output[batch_idx * NUM_HEADS * HEAD_DIM_CKV + i] = __float2bfloat16(0.0f);
        }
        return;
    }
    
    int num_chunks = (total_tokens + CHUNK_SIZE - 1) / CHUNK_SIZE;
    int valid_rows = min(CHUNK_SIZE, total_tokens);
    load_kv_chunk(smem, 0, kv_indices + page_start, valid_rows, ckv_cache, kpe_cache, tid);
    __pipeline_commit();
    
    // Fragments for S computation
    wmma::fragment<wmma::matrix_a, 16, 16, 16, __nv_bfloat16, wmma::row_major> frag_q;
    // frag_k is col_major to perform implicit transpose Q * K^T
    wmma::fragment<wmma::matrix_b, 16, 16, 16, __nv_bfloat16, wmma::col_major> frag_k;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> acc_s[4]; 

    // Fragments for O computation
    wmma::fragment<wmma::matrix_a, 16, 16, 16, __nv_bfloat16, wmma::row_major> frag_p;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, __nv_bfloat16, wmma::row_major> frag_v;

    for (int step = 0; step < num_chunks; ++step) {
        int buf_idx = step % 2;
        int next_step = step + 1;
        
        __pipeline_wait_prior(0);
        
        zero_invalid_rows(smem, buf_idx, valid_rows, tid);
        __syncthreads();
        
        if (next_step < num_chunks) {
            int next_valid = min(CHUNK_SIZE, total_tokens - next_step * CHUNK_SIZE);
            load_kv_chunk(smem, next_step % 2, kv_indices + page_start + next_step * CHUNK_SIZE, next_valid, ckv_cache, kpe_cache, tid);
            __pipeline_commit();
        }

        // --- 1. Compute Scores (Split-K) ---
        #pragma unroll
        for (int i=0; i<4; ++i) wmma::fill_fragment(acc_s[i], 0.0f);
        
        int k_start = warp_id * 64; 
        
        #pragma unroll
        for (int t = 0; t < 4; ++t) {
            int t_offset = t * 16;
            // CKV Part
            #pragma unroll
            for (int k = 0; k < 4; ++k) {
                int k_curr = k_start + k * 16;
                wmma::load_matrix_sync(frag_q, smem.q_nope + k_curr, HEAD_DIM_CKV);
                // Load K transposed: B = K^T.
                // We point to K[t_offset, k_curr]. Since frag_k is col_major,
                // it treats memory as column major, effectively loading rows as columns -> transpose.
                // Stride is STRIDE_CKV (rows of K).
                wmma::load_matrix_sync(frag_k, smem.kc_buf[buf_idx] + t_offset * STRIDE_CKV + k_curr, STRIDE_CKV);
                wmma::mma_sync(acc_s[t], frag_q, frag_k, acc_s[t]);
            }
            // KPE Part (Warp 0 only)
            if (warp_id == 0) {
                #pragma unroll
                for (int k = 0; k < 4; ++k) {
                    int k_curr = k * 16;
                    wmma::load_matrix_sync(frag_q, smem.q_pe + k_curr, HEAD_DIM_KPE);
                    wmma::load_matrix_sync(frag_k, smem.kp_buf[buf_idx] + t_offset * STRIDE_KPE + k_curr, STRIDE_KPE);
                    wmma::mma_sync(acc_s[t], frag_q, frag_k, acc_s[t]);
                }
            }
        }
        
        #pragma unroll
        for (int t = 0; t < 4; ++t) {
            float* dst = smem.scratch.s_partials[warp_id] + t * 16;
            wmma::store_matrix_sync(dst, acc_s[t], S_STRIDE, wmma::mem_row_major);
        }
        __syncthreads();
        
        // --- 2. Reduce S & Softmax ---
        #pragma unroll
        for (int i = 0; i < 4; ++i) {
            int idx = tid + i * BLOCK_SIZE; // 0..1023
            int h = idx >> 6; // / 64
            int c = idx & 63; // % 64
            
            float sum = 0.0f;
            int offset = h * S_STRIDE + c;
            #pragma unroll
            for (int w = 0; w < NUM_WARPS; ++w) {
                sum += smem.scratch.s_partials[w][offset];
            }
            
            if (step * CHUNK_SIZE + c >= total_tokens) {
                sum = -INFINITY;
            } else {
                sum *= sm_scale;
            }
            // Store reduced sum in warp 0 slot
            smem.scratch.s_partials[0][offset] = sum;
        }
        __syncthreads();
        
        int row = tid / 16; // 0..15
        int col_lane = tid % 16;
        
        if (row < 16) {
            float max_val = -INFINITY;
            for (int k = col_lane; k < 64; k += 16) {
                max_val = max(max_val, smem.scratch.s_partials[0][row * S_STRIDE + k]);
            }
            #pragma unroll
            for (int mask = 8; mask > 0; mask /= 2)
                max_val = max(max_val, __shfl_xor_sync(0xffffffff, max_val, mask));
            
            if (col_lane == 0) {
                float prev_max = smem.lse_max[row];
                float cur_max = max(prev_max, max_val);
                float alpha = (prev_max == -INFINITY) ? 0.0f : expf(prev_max - cur_max);
                if (prev_max == -INFINITY && cur_max == -INFINITY) alpha = 1.0f;
                
                smem.lse_max[row] = cur_max;
                smem.broadcast_alpha[row] = alpha;
            }
        }
        __syncthreads();

        if (row < 16) {
            float cur_max = smem.lse_max[row];
            float sum_p = 0.0f;
            
            for (int k = col_lane; k < 64; k += 16) {
                float val = smem.scratch.s_partials[0][row * S_STRIDE + k];
                float p;
                if (cur_max == -INFINITY) {
                    p = 0.0f;
                } else {
                    p = expf(val - cur_max);
                }
                sum_p += p;
                smem.p_mat[row * 64 + k] = __float2bfloat16(p);
            }
            #pragma unroll
            for (int mask = 8; mask > 0; mask /= 2)
                sum_p += __shfl_xor_sync(0xffffffff, sum_p, mask);
            
            if (col_lane == 0) {
                float alpha = smem.broadcast_alpha[row];
                smem.lse_sum[row] = smem.lse_sum[row] * alpha + sum_p;
            }
        }
        __syncthreads();
        
        // --- 3. Rescale Output Accumulator ---
        int warp_col_start = warp_id * 64;
        #pragma unroll
        for (int k = 0; k < 4; ++k) {
             wmma::store_matrix_sync(smem.scratch.o_exchange + warp_col_start + k * 16, acc_o[k], HEAD_DIM_CKV, wmma::mem_row_major);
        }
        __syncthreads();
        
        #pragma unroll
        for (int i = 0; i < 32; ++i) {
            int idx = tid + i * BLOCK_SIZE;
            if (idx < 16 * HEAD_DIM_CKV) {
                int r = idx / HEAD_DIM_CKV;
                float alpha = smem.broadcast_alpha[r];
                smem.scratch.o_exchange[idx] *= alpha;
            }
        }
        __syncthreads();
        
        #pragma unroll
        for (int k = 0; k < 4; ++k) {
             wmma::load_matrix_sync(acc_o[k], smem.scratch.o_exchange + warp_col_start + k * 16, HEAD_DIM_CKV, wmma::mem_row_major);
        }
        
        // --- 4. Accumulate P * V ---
        #pragma unroll
        for (int t = 0; t < 4; ++t) {
            int t_offset = t * 16;
            wmma::load_matrix_sync(frag_p, smem.p_mat + t_offset, 64);
            
            #pragma unroll
            for (int k = 0; k < 4; ++k) {
                int col_sub = warp_col_start + k * 16;
                wmma::load_matrix_sync(frag_v, smem.kc_buf[buf_idx] + t_offset * STRIDE_CKV + col_sub, STRIDE_CKV);
                wmma::mma_sync(acc_o[k], frag_p, frag_v, acc_o[k]);
            }
        }
        __syncthreads();
        
        if (next_step < num_chunks) {
            valid_rows = min(CHUNK_SIZE, total_tokens - next_step * CHUNK_SIZE);
        }
    }
    
    // --- Epilogue ---
    int warp_col_start = warp_id * 64;
    #pragma unroll
    for (int k = 0; k < 4; ++k) {
         wmma::store_matrix_sync(smem.scratch.o_exchange + warp_col_start + k * 16, acc_o[k], HEAD_DIM_CKV, wmma::mem_row_major);
    }
    __syncthreads();
    
    __nv_bfloat16* out_ptr = output + batch_idx * NUM_HEADS * HEAD_DIM_CKV;
    
    #pragma unroll
    for (int i = 0; i < 32; ++i) {
        int idx = tid + i * BLOCK_SIZE;
        if (idx < 16 * HEAD_DIM_CKV) {
            int r = idx / HEAD_DIM_CKV;
            float val = smem.scratch.o_exchange[idx];
            float sum = smem.lse_sum[r];
            float res = (sum == 0.0f) ? 0.0f : (val / sum);
            out_ptr[idx] = __float2bfloat16(res);
        }
    }
    
    if (tid < NUM_HEADS) {
        float val = logf(smem.lse_sum[tid]) + smem.lse_max[tid];
        if (smem.lse_sum[tid] == 0.0f) val = -INFINITY;
        lse[batch_idx * NUM_HEADS + tid] = val * 1.44269504f; 
    }
}

void run_mla_decode(
    torch::Tensor q_nope,
    torch::Tensor q_pe,
    torch::Tensor ckv_cache,
    torch::Tensor kpe_cache,
    torch::Tensor kv_indptr,
    torch::Tensor kv_indices,
    torch::Tensor output,
    torch::Tensor lse,
    float sm_scale
) {
    int batch_size = q_nope.size(0);
    dim3 grid(1, batch_size);
    dim3 block(256);
    
    size_t smem_size = sizeof(SharedStorage);
    cudaFuncSetAttribute(mla_decode_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
    
    mla_decode_kernel<<<grid, block, smem_size>>>(
        reinterpret_cast<__nv_bfloat16*>(q_nope.data_ptr<at::BFloat16>()),
        reinterpret_cast<__nv_bfloat16*>(q_pe.data_ptr<at::BFloat16>()),
        reinterpret_cast<__nv_bfloat16*>(ckv_cache.data_ptr<at::BFloat16>()),
        reinterpret_cast<__nv_bfloat16*>(kpe_cache.data_ptr<at::BFloat16>()),
        kv_indptr.data_ptr<int>(),
        kv_indices.data_ptr<int>(),
        reinterpret_cast<__nv_bfloat16*>(output.data_ptr<at::BFloat16>()),
        lse.data_ptr<float>(),
        sm_scale
    );
}
</cuda_file>

<cpp_file name="main.cpp">
#include <torch/extension.h>
#include <pybind11/stl.h>
#include <cuda_runtime.h>
#include "kernel.h"

std::map<std::string, torch::Tensor> run_dict(
    torch::Tensor q_nope,
    torch::Tensor q_pe,
    torch::Tensor ckv_cache,
    torch::Tensor kpe_cache,
    torch::Tensor kv_indptr,
    torch::Tensor kv_indices,
    float sm_scale
) {
    // Ensure contiguous inputs for raw pointer access
    q_nope = q_nope.contiguous();
    q_pe = q_pe.contiguous();
    ckv_cache = ckv_cache.contiguous();
    kpe_cache = kpe_cache.contiguous();
    kv_indptr = kv_indptr.contiguous();
    kv_indices = kv_indices.contiguous();

    int batch_size = q_nope.size(0);
    int num_heads = q_nope.size(1);
    int head_dim = q_nope.size(2);
    
    auto options = q_nope.options();
    auto output = torch::zeros({batch_size, num_heads, head_dim}, options);
    auto lse = torch::empty({batch_size, num_heads}, options.dtype(torch::kFloat32));
    
    auto kv_indptr_i32 = kv_indptr.to(torch::kInt);
    auto kv_indices_i32 = kv_indices.to(torch::kInt);

    run_mla_decode(q_nope, q_pe, ckv_cache, kpe_cache, kv_indptr_i32, kv_indices_i32, output, lse, sm_scale);
    
    std::map<std::string, torch::Tensor> result;
    result["output"] = output;
    result["lse"] = lse;
    return result;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("run", &run_dict, "MLA Decode Kernel",
          py::arg("q_nope"), py::arg("q_pe"), 
          py::arg("ckv_cache"), py::arg("kpe_cache"), 
          py::arg("kv_indptr"), py::arg("kv_indices"), 
          py::arg("sm_scale"));
}
</cpp_file>

# EVOLVE-BLOCK-END